{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c189ceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tabulate is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script strip-hints is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script jsonschema is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 1.12.11 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip3 install -U google-cloud-storage {USER_FLAG} -q\n",
    "! pip3 install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b638df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d8f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b8f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "project_id = \"de2022-rrd\"\n",
    "# The region that this pipeline runs in\n",
    "region = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "pipeline_root_path = \"gs://dtemp_a1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4e0b7",
   "metadata": {},
   "source": [
    "# Pipeline component: Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2ae6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def download_data(project_id: str, bucket: str, file_name: str) -> Dict:\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Downloaing the file from a google bucket \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    local_path = '/tmp/'+ file_name\n",
    "    blob.download_to_filename(local_path)\n",
    "    logging.info('Downloaded Data!')\n",
    "    \n",
    "    # Convert the data to a dictiory object    \n",
    "    dict_from_csv = pd.read_csv(local_path, index_col=None, squeeze=True).to_dict()\n",
    "    logging.info('Returning Data as Dictionary Object!')\n",
    "    return dict_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad08777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for data ingestion\n",
    "data_ingestion_component = kfp.components.create_component_from_func(\n",
    "    download_data, output_component_file='data_ingestion.yaml', packages_to_install=['google-cloud-storage', 'pandas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdd7cd",
   "metadata": {},
   "source": [
    "# Pipeline component: Training SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42516385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Dict\n",
    "\n",
    "def train_svc (features: Dict, project_id: str, model_repo: str ) -> Dict:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    import joblib\n",
    "    from google.cloud import storage\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # reading data\n",
    "    df = pd.DataFrame.from_dict(features)\n",
    "    cat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\n",
    "    con_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n",
    "\n",
    "    # creating copy of df and encode categorical columns\n",
    "    df1 = df\n",
    "    df1 = pd.get_dummies(df1, columns = cat_cols, drop_first = True)\n",
    "\n",
    "    # defining features and target\n",
    "    y = df1[['output']]\n",
    "    X = df1.drop(['output'],axis=1)\n",
    "\n",
    "    # instantiate the scaler\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # scaling the continuous features\n",
    "    X[con_cols] = scaler.fit_transform(X[con_cols])\n",
    "\n",
    "    # train test split and fit\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)\n",
    "    clf = SVC(kernel='linear', C=1, random_state=42).fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Accuracy logging\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred)\n",
    "    }\n",
    "    logging.info(metrics)\n",
    "\n",
    "    # Save the model localy\n",
    "    local_file = '/tmp/local_model.h5'\n",
    "    joblib.dump(clf, local_file)\n",
    "\n",
    "    # Save to GCS as model.pkl\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('svc_model.h5')\n",
    "    # Upload the locally saved model\n",
    "    blob.upload_from_filename(local_file)\n",
    "\n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5727435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for training SVC model\n",
    "train_svc_component = kfp.components.create_component_from_func(\n",
    "    train_svc, output_component_file='train_svc_model.yaml', packages_to_install=['google-cloud-storage', 'tensorflow' ,'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b535721d",
   "metadata": {},
   "source": [
    "# Pipeline Component: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21a9bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Dict\n",
    "\n",
    "def train_LR (features: Dict, project_id: str, model_repo: str ) -> Dict:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    import joblib\n",
    "    from google.cloud import storage\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # reading data\n",
    "    df = pd.DataFrame.from_dict(features)\n",
    "    cat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\n",
    "    con_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n",
    "\n",
    "    # creating copy of df and encode categorical columns\n",
    "    df1 = df\n",
    "    df1 = pd.get_dummies(df1, columns = cat_cols, drop_first = True)\n",
    "\n",
    "    # defining features and target\n",
    "    y = df1[['output']]\n",
    "    X = df1.drop(['output'],axis=1)\n",
    "\n",
    "    # instantiate the scaler\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # scaling the continuous features\n",
    "    X[con_cols] = scaler.fit_transform(X[con_cols])\n",
    "\n",
    "    # train test split and fit\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)\n",
    "    \n",
    "    # instantiating the object\n",
    "    logreg = LogisticRegression()\n",
    "\n",
    "    # fitting the object\n",
    "    logreg.fit(X_train, y_train)\n",
    "    \n",
    "    # calculating the probabilities\n",
    "    y_pred_proba = logreg.predict_proba(X_test)\n",
    "\n",
    "    # finding the predicted valued\n",
    "    y_pred = np.argmax(y_pred_proba,axis=1)\n",
    "\n",
    "    # Accuracy logging\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred)\n",
    "    }\n",
    "    logging.info(metrics)\n",
    "\n",
    "    # Save the model localy\n",
    "    local_file = '/tmp/local_model.h5'\n",
    "    joblib.dump(logreg, local_file)\n",
    "\n",
    "    # Save to GCS as model.pkl\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('logreg_model.h5')\n",
    "    # Upload the locally saved model\n",
    "    blob.upload_from_filename(local_file)\n",
    "\n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "680b8b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for training logreg model\n",
    "train_lr_component = kfp.components.create_component_from_func(\n",
    "    train_LR, output_component_file='train_LR_model.yaml', packages_to_install=['google-cloud-storage', 'pandas', \n",
    "                                                                                'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519547e",
   "metadata": {},
   "source": [
    "# Model comparison component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a5de3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Dict\n",
    "\n",
    "def compare_model_result(svc_metrics: Dict, lr_metrics: Dict) -> str:\n",
    "    import logging\n",
    "    import json\n",
    "    import sys\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(svc_metrics)\n",
    "    logging.info(lr_metrics)\n",
    "    if svc_metrics.get(\"accuracy\") > lr_metrics.get(\"accuracy\"):\n",
    "        return \"SVC\"\n",
    "    else :\n",
    "        return \"LR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "993949da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# component for selecting between SVC and LR\n",
    "compare_model_comp = kfp.components.create_component_from_func(\n",
    "    compare_model_result, output_component_file='model_comparison_result.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca55a70",
   "metadata": {},
   "source": [
    "# Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fa7c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"heart-attack-training-pipeline\",\n",
    "    pipeline_root=pipeline_root_path)\n",
    "\n",
    "def pipeline(project_id: str, data_bucket: str, dataset_filename: str, model_repo: str, ):\n",
    "    \n",
    "    data_ingestion_op = data_ingestion_component(\n",
    "    project_id = project_id,\n",
    "    bucket = data_bucket,\n",
    "    file_name = dataset_filename\n",
    "    )\n",
    "    \n",
    "    training_svc_job_run_op = train_svc_component(\n",
    "    project_id = project_id,\n",
    "    model_repo = model_repo,\n",
    "    features = data_ingestion_op.output\n",
    "    )\n",
    "    \n",
    "    training_lr_job_run_op = train_lr_component(\n",
    "    project_id = project_id,\n",
    "    model_repo = model_repo,\n",
    "    features = data_ingestion_op.output\n",
    "    )\n",
    "    \n",
    "    compare_model_op = compare_model_comp(training_svc_job_run_op.output,\n",
    "                                   training_lr_job_run_op.output).after(training_svc_job_run_op, training_lr_job_run_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb5b2e",
   "metadata": {},
   "source": [
    "# Compile pipeline to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cf041a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='heart_attack_training_pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29071534",
   "metadata": {},
   "source": [
    "# Submit pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "688bc778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/539022204835/locations/us-central1/pipelineJobs/heart-attack-training-pipeline-20221025201300\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/539022204835/locations/us-central1/pipelineJobs/heart-attack-training-pipeline-20221025201300')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/heart-attack-training-pipeline-20221025201300?project=539022204835\n",
      "PipelineJob projects/539022204835/locations/us-central1/pipelineJobs/heart-attack-training-pipeline-20221025201300 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/539022204835/locations/us-central1/pipelineJobs/heart-attack-training-pipeline-20221025201300 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/539022204835/locations/us-central1/pipelineJobs/heart-attack-training-pipeline-20221025201300 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/539022204835/locations/us-central1/pipelineJobs/heart-attack-training-pipeline-20221025201300 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/539022204835/locations/us-central1/pipelineJobs/heart-attack-training-pipeline-20221025201300 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/539022204835/locations/us-central1/pipelineJobs/heart-attack-training-pipeline-20221025201300 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/539022204835/locations/us-central1/pipelineJobs/heart-attack-training-pipeline-20221025201300\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"heart_attack_training_pipeline\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"heart_attack_training_pipeline.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id, \n",
    "        'data_bucket': 'data_a1',  \n",
    "        'dataset_filename': 'heart.csv',     \n",
    "        'model_repo':'dmodel_a1'\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
