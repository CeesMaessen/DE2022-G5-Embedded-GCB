{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c189ceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\ceesm\\\\anaconda3\\\\Lib\\\\site-packages\\\\~aml\\\\_yaml.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip3 install -U google-cloud-storage {USER_FLAG} -q\n",
    "! pip3 install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b638df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d8f226",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kfp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21964\\3243798307.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkfp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdsl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdsl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m from kfp.v2.dsl import (\n\u001b[0;32m      5\u001b[0m     \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kfp'"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "project_id = \"data-engineering-jads\"\n",
    "# The region that this pipeline runs in\n",
    "region = \"us-west1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "pipeline_root_path = \"gs://jads_temp695341\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4e0b7",
   "metadata": {},
   "source": [
    "# Pipeline component: Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ae6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def download_data(project_id: str, bucket: str, file_name: str) -> Dict:\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Downloaing the file from a google bucket \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    local_path = '/tmp/'+ file_name\n",
    "    blob.download_to_filename(local_path)\n",
    "    logging.info('Downloaded Data!')\n",
    "    \n",
    "    # Convert the data to a dictiory object    \n",
    "    dict_from_csv = pd.read_csv(local_path, index_col=None, squeeze=True).to_dict()\n",
    "    logging.info('Returning Data as Dictionary Object!')\n",
    "    return dict_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for data ingestion\n",
    "data_ingestion_component = kfp.components.create_component_from_func(\n",
    "    download_data, output_component_file='data_ingestion.yaml', packages_to_install=['google-cloud-storage', 'pandas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdd7cd",
   "metadata": {},
   "source": [
    "# Pipeline component: Training SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42516385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Dict\n",
    "\n",
    "def train_SVC (features: Dict, project_id: str, model_repo: str ) -> Dict:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    import joblib\n",
    "    from google.cloud import storage\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # reading data\n",
    "    df = pd.DataFrame.from_dict(features)\n",
    "    cat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\n",
    "    con_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n",
    "\n",
    "    # creating copy of df and encode categorical columns\n",
    "    df1 = df\n",
    "    df1 = pd.get_dummies(df1, columns = cat_cols, drop_first = True)\n",
    "\n",
    "    # defining features and target\n",
    "    y = df1[['output']]\n",
    "    X = df1.drop(['output'],axis=1)\n",
    "\n",
    "    # instantiate the scaler\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # scaling the continuous features\n",
    "    X[con_cols] = scaler.fit_transform(X[con_cols])\n",
    "\n",
    "    # train test split and fit\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)\n",
    "    clf = SVC(kernel='linear', C=1, random_state=42).fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Accuracy logging\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred)\n",
    "    }\n",
    "    logging.info(metrics)\n",
    "\n",
    "    # Save the model localy\n",
    "    local_file = '/tmp/local_model.pkl'\n",
    "    joblib.dump(clf, local_file)\n",
    "\n",
    "    # Save to GCS as model.h5\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('svc_model.pkl')\n",
    "    # Upload the locally saved model\n",
    "    blob.upload_from_filename(local_file)\n",
    "\n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5727435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for training SVC model\n",
    "train_svc_component = kfp.components.create_component_from_func(\n",
    "    train_svc, output_component_file='train_svc_model.yaml', packages_to_install=['google-cloud-storage', 'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b535721d",
   "metadata": {},
   "source": [
    "# Pipeline Component: Prediction SVC. Not used (yet?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a9bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_svc(project_id: str, model_repo: str, features: Dict) -> Dict:\n",
    "#     import pandas as pd\n",
    "#     import joblib\n",
    "#     from google.cloud import storage\n",
    "#     import json\n",
    "#     import logging\n",
    "#     import sys\n",
    "#     import os\n",
    "\n",
    "#     logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "#     df = pd.DataFrame.from_dict(features)\n",
    "#     client = storage.Client(project=project_id)\n",
    "#     bucket = client.get_bucket(model_repo)\n",
    "#     blob = bucket.blob('svc_model.pkl')\n",
    "#     filename = '/tmp/local_model.pkl'\n",
    "#     blob.download_to_filename(filename)\n",
    "\n",
    "#     cat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\n",
    "#     con_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n",
    "\n",
    "#     #Loading the saved model with joblib\n",
    "#     model = joblib.load(filename)\n",
    "\n",
    "#     xNew = df[[\"age\", \"sex\", \"cp\", \"trtbps\", \"chol\", \"fbs\", \"restecg\", \"thalachh\", \"exng\", \"oldpeak\", \"slp\", \"caa\",\n",
    "#                \"thall\"]]\n",
    "\n",
    "#     # making dummy vars for categorical variables\n",
    "#     xNew = pd.get_dummies(xNew, columns = cat_cols, drop_first = True)\n",
    "\n",
    "#     # instantiate the scaler\n",
    "#     scaler = RobustScaler()\n",
    "\n",
    "#     # scaling the continuous features\n",
    "#     XNew[con_cols] = scaler.fit_transform(XNew[con_cols])\n",
    "\n",
    "#     df_copy = df.copy()\n",
    "#     y_class = model.predict(xNew)\n",
    "#     logging.info(y_class)\n",
    "#     df_copy['output'] = y_class.tolist()\n",
    "#     dict = dfcp.to_dict(orient = 'records')\n",
    "#     return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b8b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a KFP component for prediction SVC\n",
    "# prediction_svc_component = kfp.components.create_component_from_func(\n",
    "#     predict_svc, output_component_file='prediction_svc_com.yaml', packages_to_install=['google-cloud-storage', 'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca55a70",
   "metadata": {},
   "source": [
    "# Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"heart-attack-training-pipeline\",\n",
    "    pipeline_root=pipeline_root_path)\n",
    "\n",
    "def pipeline(project_id: str, data_bucket: str, dataset_filename: str, model_repo: str, ):\n",
    "    \n",
    "    data_ingestion_op = data_ingestion_component(\n",
    "    project_id = project_id,\n",
    "    bucket = data_bucket,\n",
    "    file_name = dataset_filename\n",
    "    )\n",
    "    \n",
    "    training_svc_job_run_op = train_svc_component(\n",
    "    project_id = project_id,\n",
    "    model_repo = model_repo,\n",
    "    features = data_ingestion_op.output\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb5b2e",
   "metadata": {},
   "source": [
    "# Compile pipeline to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf041a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='heart_attack_training_pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29071534",
   "metadata": {},
   "source": [
    "# Submit pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688bc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"heart_attack_training_pipeline\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"heart_attack_training_pipeline.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id, \n",
    "        'data_bucket': 'data_695341',  \n",
    "        'dataset_filename': 'heart.csv',     \n",
    "        'model_repo':'model_repo_de695341'\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
